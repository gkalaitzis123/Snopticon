{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb711c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "from pathlib import Path\n",
    "\n",
    "# === SETTINGS ===\n",
    "video_output_folder = \"snakeTrain2\"\n",
    "image_output_folder = \"snakeDataset2\"\n",
    "os.makedirs(video_output_folder, exist_ok=True)\n",
    "os.makedirs(image_output_folder, exist_ok=True)\n",
    "\n",
    "fps = 15.0\n",
    "cooldown_seconds = 2\n",
    "min_motion_area = 50\n",
    "min_record_duration = 10  # seconds\n",
    "frame_size = (1280, 720)\n",
    "\n",
    "\n",
    "def adjust_brightness(cap):\n",
    "    hour = datetime.now().hour\n",
    "    cap.set(cv2.CAP_PROP_BRIGHTNESS, 0.2 if hour >= 22 or hour < 9 else 1.0)\n",
    "    return \"night\" if hour >= 22 or hour < 9 else \"day\"\n",
    "\n",
    "\n",
    "def draw_timestamp_overlay(frame, brightness_mode, cam_name):\n",
    "    now = datetime.now()\n",
    "    datetime_str = now.strftime(\"%m/%d/%y %H:%M:%S\")\n",
    "    label = \"Day\" if brightness_mode == \"day\" else \"Night\"\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 0.6\n",
    "    color = (0, 0, 255)\n",
    "    thickness = 2\n",
    "\n",
    "    lines = [cam_name.upper(), datetime_str, label]\n",
    "    y_offset = 25\n",
    "    for i, line in enumerate(lines):\n",
    "        size = cv2.getTextSize(line, font, scale, thickness)[0]\n",
    "        x = frame.shape[1] - size[0] - 10\n",
    "        y = (i + 1) * y_offset\n",
    "        cv2.putText(frame, line, (x, y), font, scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def extract_training_frames(video_path, base_name):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame25 = int(total_frames * 0.25)\n",
    "    frame75 = int(total_frames * 0.75)\n",
    "\n",
    "    for percent, frame_id in zip([\"25\", \"75\"], [frame25, frame75]):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            out_filename = f\"{base_name}_{percent}.jpg\"\n",
    "            out_path = os.path.join(image_output_folder, out_filename)\n",
    "            cv2.imwrite(out_path, frame)\n",
    "            print(f\"âœ… Saved training image: {out_filename}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed to extract {percent}% frame from {base_name}\")\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "def monitor_camera(camera_index, cam_name):\n",
    "    def init_camera(index):\n",
    "        cam = cv2.VideoCapture(index)\n",
    "        cam.set(cv2.CAP_PROP_FRAME_WIDTH, frame_size[0])\n",
    "        cam.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_size[1])\n",
    "        cam.set(cv2.CAP_PROP_FPS, fps)\n",
    "        return cam\n",
    "\n",
    "    cap = init_camera(camera_index)\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"[{cam_name}] âŒ Failed to access camera.\")\n",
    "        return\n",
    "\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    prev_gray = cv2.GaussianBlur(prev_gray, (21, 21), 0)\n",
    "\n",
    "    motion_active = False\n",
    "    last_motion_time = None\n",
    "    motion_start_time = None\n",
    "    clip_counter = 0\n",
    "    video_writer = None\n",
    "\n",
    "    print(f\"[{cam_name}] Watching for motion...\")\n",
    "\n",
    "    while True:\n",
    "        brightness_mode = adjust_brightness(cap)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"[{cam_name}] âŒ Disconnected. Reinitializing...\")\n",
    "            cap.release()\n",
    "            time.sleep(1)\n",
    "            cap = init_camera(camera_index)\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "        delta = cv2.absdiff(prev_gray, gray)\n",
    "        thresh = cv2.threshold(delta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "        contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        motion_detected = any(cv2.contourArea(c) > min_motion_area for c in contours)\n",
    "        current_time = time.time()\n",
    "\n",
    "        frame = draw_timestamp_overlay(frame, brightness_mode, cam_name)\n",
    "\n",
    "        if motion_detected:\n",
    "            last_motion_time = current_time\n",
    "            if not motion_active:\n",
    "                clip_counter += 1\n",
    "                filename = os.path.join(video_output_folder, f\"{cam_name}_clip_{clip_counter:04d}.avi\")\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "                video_writer = cv2.VideoWriter(filename, fourcc, fps, frame_size)\n",
    "                motion_start_time = current_time\n",
    "                motion_active = True\n",
    "                print(f\"[{cam_name}] ðŸ”´ Recording started: {filename}\")\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        elif motion_active:\n",
    "            elapsed = current_time - motion_start_time\n",
    "            video_writer.write(frame)\n",
    "\n",
    "            if elapsed >= min_record_duration and (current_time - last_motion_time > cooldown_seconds):\n",
    "                video_writer.release()\n",
    "                print(f\"[{cam_name}] âœ… Clip saved ({elapsed:.1f}s)\")\n",
    "                motion_active = False\n",
    "                video_writer = None\n",
    "\n",
    "                base_name = f\"{cam_name}_clip_{clip_counter:04d}\"\n",
    "                extract_training_frames(filename, base_name)\n",
    "\n",
    "        prev_gray = gray\n",
    "\n",
    "        cv2.imshow(f\"{cam_name}\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "    cap.release()\n",
    "    print(f\"[{cam_name}] Camera released.\")\n",
    "\n",
    "\n",
    "# === Start Threads for Both Cameras ===\n",
    "thread1 = Thread(target=monitor_camera, args=(0, 'CAM 1'))\n",
    "thread2 = Thread(target=monitor_camera, args=(1, 'CAM 2'))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "print(\"ðŸŽ‰ All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f2a82-c44e-476d-adc9-5315ef112355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND TEST SNOPT\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"z8o24Csri2knE8jtx3g8\")\n",
    "project = rf.workspace(\"gkala\").project(\"snopticon\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "# Load a pretrained model (e.g., yolov8n.pt for nano, yolov8s.pt for small)\n",
    "model = YOLO(\"yolov8n.pt\")  # You can also use yolov8s.pt, yolov8m.pt, etc.\n",
    "\n",
    "# Train\n",
    "model.train(data=dataset.location + \"/data.yaml\",epochs=50,imgsz=640,batch=8, val=False)\n",
    "\n",
    "# Path to your test images (inside the Roboflow dataset folder)\n",
    "test_images_path = os.path.join(dataset.location, \"test\", \"images\")\n",
    "\n",
    "# Run prediction on the entire test set\n",
    "results = model.predict(source=test_images_path,save=True,save_txt=True,conf=0.25,imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN INFERENCE ON VIDEO USING SNOPT\n",
    "\n",
    "# Load trained model\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")  # or \"yolov8n.pt\"\n",
    "\n",
    "# Path to input video\n",
    "video_path = \"snake_clip.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Output writer (optional - saves annotated video)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(\"output_snake.avi\", fourcc, 30.0,\n",
    "                      (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, imgsz=640, conf=0.25)\n",
    "\n",
    "    # Visualize predictions on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Show frame with bounding boxes\n",
    "    cv2.imshow(\"YOLOv8 Snake Detection\", annotated_frame)\n",
    "\n",
    "    # Save to file\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
